{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from proj1_helpers import *\n",
    "from Hugues import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "labels, raw_data, indices = load_csv_data('train.csv', sub_sample=False)\n",
    "\n",
    "labels_te, raw_data_te, indices_te = load_csv_data('test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0 jet (99913, 30) , 1 jet (77544, 30) , 2 jet (72543, 30)\n",
      "Test: 0 jet (227458, 30) , 1 jet (175338, 30) , 2 jet (165442, 30)\n"
     ]
    }
   ],
   "source": [
    "labels0, data0, labels1, data1, labels2, data2 = divide_data(labels, raw_data)\n",
    "labels0_te, data0_te, labels1_te, data1_te, labels2_te, data2_te = divide_data(labels_te, raw_data_te)\n",
    "\n",
    "print('Train:', '0 jet', data0.shape, ', 1 jet', data1.shape, ', 2 jet', data2.shape)\n",
    "print('Test:', '0 jet', data0_te.shape, ', 1 jet', data1_te.shape, ', 2 jet', data2_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data0, mean0, std0 = standardize_train(data0)\n",
    "std_data0_te = standardize_test(data0_te, mean0, std0)\n",
    "\n",
    "std_data1, mean1, std1 = standardize_train(data1)\n",
    "std_data1_te = standardize_test(data1_te, mean1, std1)\n",
    "\n",
    "std_data2, mean2, std2 = standardize_train(data2)\n",
    "std_data1_te = standardize_test(data1_te, mean2, std2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estiamtion of feature 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b62c8fde883e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_estimation_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/MachineLearning/project1/Scripts/Hugues.py\u001b[0m in \u001b[0;36mcolumn_estimation_train\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mweights_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmatrix0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnan_lines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnan_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "data, weights_train = column_estimation_train(std_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate whole features\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing NaN [ 0  4  5  6 12 23 24 25 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "# Estimation of columns for train and test set\n",
    "estimated_data,nan_columns_train,weights_train = column_estimation_train(nan_data)\n",
    "estimated_data_te = column_estimation_test(nan_data_te,nan_columns_train,weights_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of train and test set\n",
    "std_data, mean_train, std_train = standardize_train(estimated_data)\n",
    "std_data_te = standardize_test(estimated_data_te, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 10 Lambda: 1e-08\n",
      "Score: 0.8122\n",
      "Loss: 0.09285\n",
      "Degree: 10 Lambda: 1e-09\n",
      "Score: 0.81216\n",
      "Loss: 0.092854\n",
      "Degree: 10 Lambda: 1e-10\n",
      "Score: 0.81216\n",
      "Loss: 0.09285600000000001\n",
      "Degree: 11 Lambda: 1e-08\n",
      "Score: 0.81392\n",
      "Loss: 0.09202800000000001\n",
      "Degree: 11 Lambda: 1e-09\n",
      "Score: 0.81396\n",
      "Loss: 0.092016\n",
      "Degree: 11 Lambda: 1e-10\n",
      "Score: 0.81392\n",
      "Loss: 0.09203800000000001\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "degrees = [10,11]\n",
    "lambdas = np.logspace(-8, -10, 3)\n",
    "k_fold = 10\n",
    "seed = 42\n",
    "\n",
    "k_idx = build_k_indices(labels, k_fold, seed)\n",
    "loss_te = np.ones((len(degrees),len(lambdas)))\n",
    "scores = np.ones((len(degrees),len(lambdas)))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te[degree_idx, lambda_idx], scores[degree_idx, lambda_idx]= cross_validation_(labels, std_data, k_idx, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score:', scores[degree_idx, lambda_idx])\n",
    "        print('Loss:', loss_te[degree_idx, lambda_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree: 11 Best lambda: 1e-09 Best score: 0.81396 Best loss 0.092016\n"
     ]
    }
   ],
   "source": [
    "ratio = scores/loss_te\n",
    "best_HP_idx = np.unravel_index(np.argmax(ratio), np.shape(ratio))\n",
    "best_degree = degrees[best_HP_idx[0]]\n",
    "best_lambda = lambdas[best_HP_idx[1]]\n",
    "\n",
    "best_score = scores[best_HP_idx[0], best_HP_idx[1]]\n",
    "best_loss = loss_te[best_HP_idx[0], best_HP_idx[1]]\n",
    "\n",
    "print('Best degree:', best_degree, 'Best lambda:', best_lambda, 'Best score:', best_score, 'Best loss', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to get weights\n",
    "poly_std_data = build_poly(std_data, best_degree)\n",
    "weights, loss = ridge_regression(labels, poly_std_data, best_lambda)\n",
    "\n",
    "#Predict on test\n",
    "poly_std_data_te = build_poly(std_data_te, best_degree)\n",
    "y_pred_te = predict_labels(weights, poly_std_data_te)\n",
    "\n",
    "create_csv_submission(indices_te, y_pred_te, 'ridge_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

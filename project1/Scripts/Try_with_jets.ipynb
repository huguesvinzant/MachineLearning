{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from proj1_helpers import *\n",
    "from Hugues import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "labels, raw_data, indices = load_csv_data('train.csv', sub_sample=False)\n",
    "nan_data = meaningless_to_nan(raw_data)\n",
    "labels_te, raw_data_te, indices_te = load_csv_data('test.csv', sub_sample=False)\n",
    "nan_data_te = meaningless_to_nan(raw_data_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0 jet (99913, 30) , 1 jet (77544, 30) , 2 jet (72543, 30)\n",
      "Test: 0 jet (227458, 30) , 1 jet (175338, 30) , 2 jet (165442, 30)\n"
     ]
    }
   ],
   "source": [
    "labels0, data0, labels1, data1, labels2, data2 = divide_data(labels, raw_data)\n",
    "labels0_te, data0_te, labels1_te, data1_te, labels2_te, data2_te = divide_data(labels_te, raw_data_te)\n",
    "\n",
    "print('Train:', '0 jet', data0.shape, ', 1 jet', data1.shape, ', 2 jet', data2.shape)\n",
    "print('Test:', '0 jet', data0_te.shape, ', 1 jet', data1_te.shape, ', 2 jet', data2_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing NaN [ 4  5  6 12 23 24 25 26 27 28]\n",
      "(99913, 18)\n",
      "Columns containing NaN [ 4  5  6 12 26 27 28]\n",
      "(77544, 22)\n",
      "(72543, 29)\n"
     ]
    }
   ],
   "source": [
    "nan_columns0 = nan_find_columns(data0)\n",
    "nan_columns0 = np.append(nan_columns0, [22,29]) #because column 29 is filled only with zeros\n",
    "clean_data0 = np.delete(data0, nan_columns0, axis = 1)\n",
    "clean_data0_te = np.delete(data0_te, nan_columns0, axis = 1)\n",
    "print(clean_data0.shape)\n",
    "\n",
    "nan_columns1 = nan_find_columns(data1)\n",
    "nan_columns1 = np.append(nan_columns1, 22)\n",
    "clean_data1 = np.delete(data1, nan_columns1, axis = 1)\n",
    "clean_data1_te = np.delete(data1_te, nan_columns1, axis = 1)\n",
    "print(clean_data1.shape)\n",
    "\n",
    "#for notation\n",
    "clean_data2 = np.delete(data2, 22, axis = 1)\n",
    "clean_data2_te = np.delete(data2_te, 22, axis = 1)\n",
    "print(clean_data2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data0, mean0, std0 = standardize_train(clean_data0) \n",
    "std_data0_te = standardize_test(clean_data0_te, mean0, std0)\n",
    "\n",
    "std_data1, mean1, std1 = standardize_train(clean_data1)\n",
    "std_data1_te = standardize_test(clean_data1_te, mean1, std1)\n",
    "\n",
    "std_data2, mean2, std2 = standardize_train(clean_data2)\n",
    "std_data2_te = standardize_test(clean_data2_te, mean2, std2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of feature 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_data0, weights_train0 = column_estimation_train(std_data0)\n",
    "estimated_data0_te = column_estimation_test(std_data0_te, weights_train0)\n",
    "\n",
    "estimated_data1, weights_train1 = column_estimation_train(std_data1)\n",
    "estimated_data1_te = column_estimation_test(std_data1_te, weights_train1)\n",
    "\n",
    "estimated_data2, weights_train2 = column_estimation_train(std_data2)\n",
    "estimated_data2_te = column_estimation_test(std_data2_te, weights_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 5 Lambda: 0.1\n",
      "Score0: 0.8288459613652287\n",
      "Loss0: 0.08495646081473326\n",
      "Degree: 5 Lambda: 0.01\n",
      "Score0: 0.8326493844460015\n",
      "Loss0: 0.08269442498248424\n",
      "Degree: 5 Lambda: 0.001\n",
      "Score0: 0.8347512761485337\n",
      "Loss0: 0.08247923130817736\n",
      "Degree: 5 Lambda: 0.0001\n",
      "Score0: 0.83395055549995\n",
      "Loss0: 0.08358522670403364\n",
      "Degree: 5 Lambda: 1e-05\n",
      "Score0: 0.8343509158242418\n",
      "Loss0: 0.0822340106095486\n",
      "Degree: 5 Lambda: 1e-06\n",
      "Score0: 0.8345510959863878\n",
      "Loss0: 0.08217395656090482\n",
      "Degree: 5 Lambda: 1e-07\n",
      "Score0: 0.8345510959863878\n",
      "Loss0: 0.08219897908117305\n",
      "Degree: 5 Lambda: 1e-08\n",
      "Score0: 0.8345510959863878\n",
      "Loss0: 0.08220898808928036\n",
      "Degree: 5 Lambda: 1e-09\n",
      "Score0: 0.8345510959863878\n",
      "Loss0: 0.08219897908117305\n",
      "Degree: 5 Lambda: 1e-10\n",
      "Score0: 0.8346511860674607\n",
      "Loss0: 0.08216394755279752\n",
      "Degree: 6 Lambda: 0.1\n",
      "Score0: 0.8272445200680613\n",
      "Loss0: 0.08534180762686419\n",
      "Degree: 6 Lambda: 0.01\n",
      "Score0: 0.48593734360924834\n",
      "Loss0: 0.10876789110199178\n",
      "Degree: 6 Lambda: 0.001\n",
      "Score0: 0.8353518166349715\n",
      "Loss0: 0.085386848163347\n",
      "Degree: 6 Lambda: 0.0001\n",
      "Score0: 0.8348513662296067\n",
      "Loss0: 0.0819337403663297\n",
      "Degree: 6 Lambda: 1e-05\n",
      "Score0: 0.833850465418877\n",
      "Loss0: 0.08279951956761085\n",
      "Degree: 6 Lambda: 1e-06\n",
      "Score0: 0.8344510059053147\n",
      "Loss0: 0.08170353317986188\n",
      "Degree: 6 Lambda: 1e-07\n",
      "Score0: 0.8361525372835552\n",
      "Loss0: 0.08149834851366229\n",
      "Degree: 6 Lambda: 1e-08\n",
      "Score0: 0.83395055549995\n",
      "Loss0: 0.08159343409068161\n",
      "Degree: 6 Lambda: 1e-09\n",
      "Score0: 0.8341507356620959\n",
      "Loss0: 0.08158842958662796\n",
      "Degree: 6 Lambda: 1e-10\n",
      "Score0: 0.8270443399059153\n",
      "Loss0: 0.08192873586227604\n",
      "Degree: 7 Lambda: 0.1\n",
      "Score0: 0.8262436192573316\n",
      "Loss0: 0.08609748773896506\n",
      "Degree: 7 Lambda: 0.01\n",
      "Score0: 0.8359523571214093\n",
      "Loss0: 0.08135822240016014\n",
      "Degree: 7 Lambda: 0.001\n",
      "Score0: 0.8379541587428686\n",
      "Loss0: 0.08316484836352718\n",
      "Degree: 7 Lambda: 0.0001\n",
      "Score0: 0.83665298768892\n",
      "Loss0: 0.08078270443399059\n",
      "Degree: 7 Lambda: 1e-05\n",
      "Score0: 0.836552897607847\n",
      "Loss0: 0.08074266840156141\n",
      "Degree: 7 Lambda: 1e-06\n",
      "Score0: 0.8364528075267741\n",
      "Loss0: 0.08072265038534682\n",
      "Degree: 7 Lambda: 1e-07\n",
      "Score0: 0.836552897607847\n",
      "Loss0: 0.08085777199479532\n",
      "Degree: 7 Lambda: 1e-08\n",
      "Score0: 0.83665298768892\n",
      "Loss0: 0.0876538884996497\n",
      "Degree: 7 Lambda: 1e-09\n",
      "Score0: 0.83665298768892\n",
      "Loss0: 0.0851466319687719\n",
      "Degree: 7 Lambda: 1e-10\n",
      "Score0: 0.836853167851066\n",
      "Loss0: 0.0956360724652187\n",
      "Degree: 8 Lambda: 0.1\n",
      "Score0: 0.8288459613652287\n",
      "Loss0: 0.084931438294465\n",
      "Degree: 8 Lambda: 0.01\n",
      "Score0: 0.8379541587428686\n",
      "Loss0: 0.08022720448403564\n",
      "Degree: 8 Lambda: 0.001\n",
      "Score0: 0.7145430887799019\n",
      "Loss0: 0.08592233009708738\n",
      "Degree: 8 Lambda: 0.0001\n",
      "Score0: 0.8401561405264738\n",
      "Loss0: 0.07977179461515363\n",
      "Degree: 8 Lambda: 1e-05\n",
      "Score0: 0.8399559603643278\n",
      "Loss0: 0.07971174056650987\n",
      "Degree: 8 Lambda: 1e-06\n",
      "Score0: 0.8400560504454009\n",
      "Loss0: 0.07972174957461715\n",
      "Degree: 8 Lambda: 1e-07\n",
      "Score0: 0.8399559603643278\n",
      "Loss0: 0.07993694324892403\n",
      "Degree: 8 Lambda: 1e-08\n",
      "Score0: 0.8401561405264738\n",
      "Loss0: 0.07960664598138324\n",
      "Degree: 8 Lambda: 1e-09\n",
      "Score0: 0.8400560504454009\n",
      "Loss0: 0.0796116504854369\n",
      "Degree: 8 Lambda: 1e-10\n",
      "Score0: 0.8401561405264738\n",
      "Loss0: 0.07961665498949053\n",
      "Degree: 9 Lambda: 0.1\n",
      "Score0: 0.8243419077169453\n",
      "Loss0: 0.08501151035932339\n",
      "Degree: 9 Lambda: 0.01\n",
      "Score0: 0.8385546992293064\n",
      "Loss0: 0.08005204684215794\n",
      "Degree: 9 Lambda: 0.001\n",
      "Score0: 0.8404564107696927\n",
      "Loss0: 0.07914623160844761\n",
      "Degree: 9 Lambda: 0.0001\n",
      "Score0: 0.8393554198778901\n",
      "Loss0: 0.07917625863276949\n",
      "Degree: 9 Lambda: 1e-05\n",
      "Score0: 0.839655690121109\n",
      "Loss0: 0.0791662496246622\n",
      "Degree: 9 Lambda: 1e-06\n",
      "Score0: 0.839555600040036\n",
      "Loss0: 0.07919627664898407\n",
      "Degree: 9 Lambda: 1e-07\n",
      "Score0: 0.8401561405264738\n",
      "Loss0: 0.07927634871384245\n",
      "Degree: 9 Lambda: 1e-08\n",
      "Score0: 0.839555600040036\n",
      "Loss0: 0.0791362226003403\n",
      "Degree: 9 Lambda: 1e-09\n",
      "Score0: 0.8394555099589631\n",
      "Loss0: 0.07913121809628665\n",
      "Degree: 9 Lambda: 1e-10\n",
      "Score0: 0.8394555099589631\n",
      "Loss0: 0.07911620458412569\n",
      "Degree: 10 Lambda: 0.1\n",
      "Score0: 0.8289460514463016\n",
      "Loss0: 0.08468621759583625\n",
      "Degree: 10 Lambda: 0.01\n",
      "Score0: 0.8391552397157441\n",
      "Loss0: 0.0801821639475528\n",
      "Degree: 10 Lambda: 0.001\n",
      "Score0: 0.839655690121109\n",
      "Loss0: 0.09325392853568212\n",
      "Degree: 10 Lambda: 0.0001\n",
      "Score0: 0.8399559603643278\n",
      "Loss0: 0.08573215894304875\n",
      "Degree: 10 Lambda: 1e-05\n",
      "Score0: 0.839655690121109\n",
      "Loss0: 0.08526173556200581\n",
      "Degree: 10 Lambda: 1e-06\n",
      "Score0: 0.839755780202182\n",
      "Loss0: 0.08747372635371833\n",
      "Degree: 10 Lambda: 1e-07\n",
      "Score0: 0.8267440696626964\n",
      "Loss0: 0.08541187068361525\n",
      "Degree: 10 Lambda: 1e-08\n",
      "Score0: 0.8376538884996497\n",
      "Loss0: 0.07989690721649484\n",
      "Degree: 10 Lambda: 1e-09\n",
      "Score0: 0.8371534380942849\n",
      "Loss0: 0.07995696126513863\n",
      "Degree: 10 Lambda: 1e-10\n",
      "Score0: 0.836753077769993\n",
      "Loss0: 0.07983685316785107\n",
      "Degree: 5 Lambda: 0.1\n",
      "Score1: 0.7746969306164561\n",
      "Loss1: 0.11370260510704153\n",
      "Degree: 5 Lambda: 0.01\n",
      "Score1: 0.7823059066288367\n",
      "Loss1: 0.11081377353623936\n",
      "Degree: 5 Lambda: 0.001\n",
      "Score1: 0.7807583182873356\n",
      "Loss1: 0.11042042816610782\n",
      "Degree: 5 Lambda: 0.0001\n",
      "Score1: 0.7838534949703378\n",
      "Loss1: 0.11009801392829507\n",
      "Degree: 5 Lambda: 1e-05\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.11014315192158886\n",
      "Degree: 5 Lambda: 1e-06\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.1101238070673201\n",
      "Degree: 5 Lambda: 1e-07\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.11013025535207636\n",
      "Degree: 5 Lambda: 1e-08\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.11014315192158888\n",
      "Degree: 5 Lambda: 1e-09\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.11014960020634512\n",
      "Degree: 5 Lambda: 1e-10\n",
      "Score1: 0.7838534949703378\n",
      "Loss1: 0.1101238070673201\n",
      "Degree: 6 Lambda: 0.1\n",
      "Score1: 0.775986587567707\n",
      "Loss1: 0.11314805261800362\n",
      "Degree: 6 Lambda: 0.01\n",
      "Score1: 0.7837245292752127\n",
      "Loss1: 0.11042687645086406\n",
      "Degree: 6 Lambda: 0.001\n",
      "Score1: 0.7850141862264638\n",
      "Loss1: 0.10945963373742584\n",
      "Degree: 6 Lambda: 0.0001\n",
      "Score1: 0.7843693577508383\n",
      "Loss1: 0.10934356461181327\n",
      "Degree: 6 Lambda: 1e-05\n",
      "Score1: 0.7844983234459634\n",
      "Loss1: 0.10945318545266958\n",
      "Degree: 6 Lambda: 1e-06\n",
      "Score1: 0.7842403920557132\n",
      "Loss1: 0.10945318545266958\n",
      "Degree: 6 Lambda: 1e-07\n",
      "Score1: 0.7842403920557132\n",
      "Loss1: 0.10945318545266958\n",
      "Degree: 6 Lambda: 1e-08\n",
      "Score1: 0.7842403920557132\n",
      "Loss1: 0.10945318545266958\n",
      "Degree: 6 Lambda: 1e-09\n",
      "Score1: 0.7842403920557132\n",
      "Loss1: 0.10946608202218208\n",
      "Degree: 6 Lambda: 1e-10\n",
      "Score1: 0.7844983234459634\n",
      "Loss1: 0.10944673716791334\n",
      "Degree: 7 Lambda: 0.1\n",
      "Score1: 0.7807583182873356\n",
      "Loss1: 0.1108653598142894\n",
      "Degree: 7 Lambda: 0.01\n",
      "Score1: 0.790688676811968\n",
      "Loss1: 0.10727366520505546\n",
      "Degree: 7 Lambda: 0.001\n",
      "Score1: 0.7923652308485942\n",
      "Loss1: 0.10623549135929844\n",
      "Degree: 7 Lambda: 0.0001\n",
      "Score1: 0.7926231622388444\n",
      "Loss1: 0.10620324993551715\n",
      "Degree: 7 Lambda: 1e-05\n",
      "Score1: 0.7921072994583441\n",
      "Loss1: 0.10622259478978593\n",
      "Degree: 7 Lambda: 1e-06\n",
      "Score1: 0.7923652308485942\n",
      "Loss1: 0.10622904307454217\n",
      "Degree: 7 Lambda: 1e-07\n",
      "Score1: 0.7923652308485942\n",
      "Loss1: 0.1062096982202734\n",
      "Degree: 7 Lambda: 1e-08\n",
      "Score1: 0.7923652308485942\n",
      "Loss1: 0.1062096982202734\n",
      "Degree: 7 Lambda: 1e-09\n",
      "Score1: 0.7923652308485942\n",
      "Loss1: 0.10622904307454217\n",
      "Degree: 7 Lambda: 1e-10\n",
      "Score1: 0.7926231622388444\n",
      "Loss1: 0.1062225947897859\n",
      "Degree: 8 Lambda: 0.1\n",
      "Score1: 0.7838534949703378\n",
      "Loss1: 0.10911142636058808\n",
      "Degree: 8 Lambda: 0.01\n",
      "Score1: 0.7977817900438483\n",
      "Loss1: 0.10337245292752129\n",
      "Degree: 8 Lambda: 0.001\n",
      "Score1: 0.7985555842145989\n",
      "Loss1: 0.10156693319576993\n",
      "Degree: 8 Lambda: 0.0001\n",
      "Score1: 0.7997162754707248\n",
      "Loss1: 0.1015282434872324\n",
      "Degree: 8 Lambda: 1e-05\n",
      "Score1: 0.8006190353366005\n",
      "Loss1: 0.1016572091823575\n",
      "Degree: 8 Lambda: 1e-06\n",
      "Score1: 0.8004900696414754\n",
      "Loss1: 0.10159917461955119\n",
      "Degree: 8 Lambda: 1e-07\n",
      "Score1: 0.8004900696414754\n",
      "Loss1: 0.10160562290430745\n",
      "Degree: 8 Lambda: 1e-08\n",
      "Score1: 0.8003611039463503\n",
      "Loss1: 0.10163141604333248\n",
      "Degree: 8 Lambda: 1e-09\n",
      "Score1: 0.8003611039463503\n",
      "Loss1: 0.1016830023213825\n",
      "Degree: 8 Lambda: 1e-10\n",
      "Score1: 0.8006190353366005\n",
      "Loss1: 0.10163786432808872\n",
      "Degree: 9 Lambda: 0.1\n",
      "Score1: 0.7857879803972143\n",
      "Loss1: 0.10816997678617488\n",
      "Degree: 9 Lambda: 0.01\n",
      "Score1: 0.8030693835439773\n",
      "Loss1: 0.10110910497807583\n",
      "Degree: 9 Lambda: 0.001\n",
      "Score1: 0.8035852463244777\n",
      "Loss1: 0.099316481815837\n",
      "Degree: 9 Lambda: 0.0001\n",
      "Score1: 0.8033273149342275\n",
      "Loss1: 0.09939386123291205\n",
      "Degree: 9 Lambda: 1e-05\n",
      "Score1: 0.8031983492391024\n",
      "Loss1: 0.09943255094144957\n",
      "Degree: 9 Lambda: 1e-06\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09943899922620583\n",
      "Degree: 9 Lambda: 1e-07\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09940675780242456\n",
      "Degree: 9 Lambda: 1e-08\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.0994003095176683\n",
      "Degree: 9 Lambda: 1e-09\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09938096466339953\n",
      "Degree: 9 Lambda: 1e-10\n",
      "Score1: 0.8031983492391024\n",
      "Loss1: 0.09940030951766829\n",
      "Degree: 10 Lambda: 0.1\n",
      "Score1: 0.7881093629094661\n",
      "Loss1: 0.10778952798555583\n",
      "Degree: 10 Lambda: 0.01\n",
      "Score1: 0.8042300748001032\n",
      "Loss1: 0.10090920815063194\n",
      "Degree: 10 Lambda: 0.001\n",
      "Score1: 0.8043590404952282\n",
      "Loss1: 0.09972917204023729\n",
      "Degree: 10 Lambda: 0.0001\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09962599948413722\n",
      "Degree: 10 Lambda: 1e-05\n",
      "Score1: 0.8035852463244777\n",
      "Loss1: 0.099645344338406\n",
      "Degree: 10 Lambda: 1e-06\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09967758576218726\n",
      "Degree: 10 Lambda: 1e-07\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.099671137477431\n",
      "Degree: 10 Lambda: 1e-08\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09970982718596853\n",
      "Degree: 10 Lambda: 1e-09\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09970982718596853\n",
      "Degree: 10 Lambda: 1e-10\n",
      "Score1: 0.8029404178488522\n",
      "Loss1: 0.09969048233169978\n",
      "Degree: 5 Lambda: 0.1\n",
      "Score2: 0.7900468706920319\n",
      "Loss2: 0.10421146953405018\n",
      "Degree: 5 Lambda: 0.01\n",
      "Score2: 0.7995588640749931\n",
      "Loss2: 0.10094430658946789\n",
      "Degree: 5 Lambda: 0.001\n",
      "Score2: 0.8023159636062862\n",
      "Loss2: 0.1002481389578164\n",
      "Degree: 5 Lambda: 0.0001\n",
      "Score2: 0.8016266887234629\n",
      "Loss2: 0.10038599393438104\n",
      "Degree: 5 Lambda: 1e-05\n",
      "Score2: 0.8016266887234629\n",
      "Loss2: 0.10044113592500689\n",
      "Degree: 5 Lambda: 1e-06\n",
      "Score2: 0.8016266887234629\n",
      "Loss2: 0.10046181417149161\n",
      "Degree: 5 Lambda: 1e-07\n",
      "Score2: 0.8017645437000276\n",
      "Loss2: 0.10043424317617866\n",
      "Degree: 5 Lambda: 1e-08\n",
      "Score2: 0.8024538185828508\n",
      "Loss2: 0.10043424317617866\n",
      "Degree: 5 Lambda: 1e-09\n",
      "Score2: 0.8024538185828508\n",
      "Loss2: 0.10055141990625863\n",
      "Degree: 5 Lambda: 1e-10\n",
      "Score2: 0.8024538185828508\n",
      "Loss2: 0.10053763440860215\n",
      "Degree: 6 Lambda: 0.1\n",
      "Score2: 0.7917011304108078\n",
      "Loss2: 0.10369451337193274\n",
      "Degree: 6 Lambda: 0.01\n",
      "Score2: 0.8024538185828508\n",
      "Loss2: 0.09966914805624481\n",
      "Degree: 6 Lambda: 0.001\n",
      "Score2: 0.8034188034188035\n",
      "Loss2: 0.09923490488006616\n",
      "Degree: 6 Lambda: 0.0001\n",
      "Score2: 0.8056244830438379\n",
      "Loss2: 0.09933829611248965\n",
      "Degree: 6 Lambda: 1e-05\n",
      "Score2: 0.8059001929969671\n",
      "Loss2: 0.0993658671078026\n",
      "Degree: 6 Lambda: 1e-06\n",
      "Score2: 0.8059001929969671\n",
      "Loss2: 0.09937965260545907\n",
      "Degree: 6 Lambda: 1e-07\n",
      "Score2: 0.8061759029500964\n",
      "Loss2: 0.09940722360077199\n",
      "Degree: 6 Lambda: 1e-08\n",
      "Score2: 0.8059001929969671\n",
      "Loss2: 0.09945547284256961\n",
      "Degree: 6 Lambda: 1e-09\n",
      "Score2: 0.8056244830438379\n",
      "Loss2: 0.09946236559139784\n",
      "Degree: 6 Lambda: 1e-10\n",
      "Score2: 0.8056244830438379\n",
      "Loss2: 0.09946925834022609\n",
      "Degree: 7 Lambda: 0.1\n",
      "Score2: 0.7917011304108078\n",
      "Loss2: 0.10324648469809761\n",
      "Degree: 7 Lambda: 0.01\n",
      "Score2: 0.8027295285359801\n",
      "Loss2: 0.09950372208436724\n",
      "Degree: 7 Lambda: 0.001\n",
      "Score2: 0.8032809484422387\n",
      "Loss2: 0.09914529914529915\n",
      "Degree: 7 Lambda: 0.0001\n",
      "Score2: 0.8049352081610146\n",
      "Loss2: 0.09913840639647091\n",
      "Degree: 7 Lambda: 1e-05\n",
      "Score2: 0.8053487730907086\n",
      "Loss2: 0.09921422663358148\n",
      "Degree: 7 Lambda: 1e-06\n",
      "Score2: 0.8053487730907086\n",
      "Loss2: 0.09922801213123794\n",
      "Degree: 7 Lambda: 1e-07\n",
      "Score2: 0.8052109181141439\n",
      "Loss2: 0.09929004687069203\n",
      "Degree: 7 Lambda: 1e-08\n",
      "Score2: 0.8049352081610146\n",
      "Loss2: 0.09924869037772263\n",
      "Degree: 7 Lambda: 1e-09\n",
      "Score2: 0.8050730631375793\n",
      "Loss2: 0.09927626137303557\n",
      "Degree: 7 Lambda: 1e-10\n",
      "Score2: 0.8050730631375793\n",
      "Loss2: 0.09930383236834847\n",
      "Degree: 8 Lambda: 0.1\n",
      "Score2: 0.7963881996140061\n",
      "Loss2: 0.10117866004962779\n",
      "Degree: 8 Lambda: 0.01\n",
      "Score2: 0.8070030328094844\n",
      "Loss2: 0.09693272677143644\n",
      "Degree: 8 Lambda: 0.001\n",
      "Score2: 0.811138682106424\n",
      "Loss2: 0.09636062861869313\n",
      "Degree: 8 Lambda: 0.0001\n",
      "Score2: 0.8094844223876482\n",
      "Loss2: 0.0965260545905707\n",
      "Degree: 8 Lambda: 1e-05\n",
      "Score2: 0.8092087124345189\n",
      "Loss2: 0.09660876757650952\n",
      "Degree: 8 Lambda: 1e-06\n",
      "Score2: 0.8090708574579543\n",
      "Loss2: 0.09671905155776124\n",
      "Degree: 8 Lambda: 1e-07\n",
      "Score2: 0.8090708574579543\n",
      "Loss2: 0.09669837331127654\n",
      "Degree: 8 Lambda: 1e-08\n",
      "Score2: 0.8089330024813896\n",
      "Loss2: 0.09662944582299421\n",
      "Degree: 8 Lambda: 1e-09\n",
      "Score2: 0.8092087124345189\n",
      "Loss2: 0.09660876757650952\n",
      "Degree: 8 Lambda: 1e-10\n",
      "Score2: 0.8092087124345189\n",
      "Loss2: 0.09662944582299422\n",
      "Degree: 9 Lambda: 0.1\n",
      "Score2: 0.8014888337468983\n",
      "Loss2: 0.09867659222497933\n",
      "Degree: 9 Lambda: 0.01\n",
      "Score2: 0.8156878963330576\n",
      "Loss2: 0.09249379652605459\n",
      "Degree: 9 Lambda: 0.001\n",
      "Score2: 0.8162393162393162\n",
      "Loss2: 0.09170113041080782\n",
      "Degree: 9 Lambda: 0.0001\n",
      "Score2: 0.8161014612627516\n",
      "Loss2: 0.09179073614557484\n",
      "Degree: 9 Lambda: 1e-05\n",
      "Score2: 0.8169285911221396\n",
      "Loss2: 0.09174248690377722\n",
      "Degree: 9 Lambda: 1e-06\n",
      "Score2: 0.8172043010752689\n",
      "Loss2: 0.0916942376619796\n",
      "Degree: 9 Lambda: 1e-07\n",
      "Score2: 0.8173421560518335\n",
      "Loss2: 0.09162531017369727\n",
      "Degree: 9 Lambda: 1e-08\n",
      "Score2: 0.8172043010752689\n",
      "Loss2: 0.09162531017369727\n",
      "Degree: 9 Lambda: 1e-09\n",
      "Score2: 0.8172043010752689\n",
      "Loss2: 0.09165977391783843\n",
      "Degree: 9 Lambda: 1e-10\n",
      "Score2: 0.8170664460987042\n",
      "Loss2: 0.09168045216432312\n",
      "Degree: 10 Lambda: 0.1\n",
      "Score2: 0.8021781086297215\n",
      "Loss2: 0.0977667493796526\n",
      "Degree: 10 Lambda: 0.01\n",
      "Score2: 0.8228563551144197\n",
      "Loss2: 0.08913702784670527\n",
      "Degree: 10 Lambda: 0.001\n",
      "Score2: 0.8251998897160188\n",
      "Loss2: 0.0877309070857458\n",
      "Degree: 10 Lambda: 0.0001\n",
      "Score2: 0.8267162944582299\n",
      "Loss2: 0.08740005514199063\n",
      "Degree: 10 Lambda: 1e-05\n",
      "Score2: 0.8272677143644885\n",
      "Loss2: 0.08756548111386821\n",
      "Degree: 10 Lambda: 1e-06\n",
      "Score2: 0.8274055693410532\n",
      "Loss2: 0.08751033912324235\n",
      "Degree: 10 Lambda: 1e-07\n",
      "Score2: 0.8276812792941826\n",
      "Loss2: 0.08741384063964708\n",
      "Degree: 10 Lambda: 1e-08\n",
      "Score2: 0.8274055693410532\n",
      "Loss2: 0.08746208988144473\n",
      "Degree: 10 Lambda: 1e-09\n",
      "Score2: 0.8272677143644885\n",
      "Loss2: 0.08746898263027295\n",
      "Degree: 10 Lambda: 1e-10\n",
      "Score2: 0.8272677143644885\n",
      "Loss2: 0.08747587537910118\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "degrees = [5,6,7,8,9,10]\n",
    "lambdas = np.logspace(-1, -10, 10)\n",
    "k_fold = 10\n",
    "seed = 42\n",
    "\n",
    "k_idx0 = build_k_indices(labels0, k_fold, seed)\n",
    "loss_te0 = np.ones((len(degrees),len(lambdas)))\n",
    "scores0 = np.ones((len(degrees),len(lambdas)))\n",
    "k_idx1 = build_k_indices(labels1, k_fold, seed)\n",
    "loss_te1 = np.ones((len(degrees),len(lambdas)))\n",
    "scores1 = np.ones((len(degrees),len(lambdas)))\n",
    "k_idx2 = build_k_indices(labels2, k_fold, seed)\n",
    "loss_te2 = np.ones((len(degrees),len(lambdas)))\n",
    "scores2 = np.ones((len(degrees),len(lambdas)))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te0[degree_idx, lambda_idx], scores0[degree_idx, lambda_idx]= cross_validation(labels0, estimated_data0, k_idx0, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score0:', scores0[degree_idx, lambda_idx])\n",
    "        print('Loss0:', loss_te0[degree_idx, lambda_idx])\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te1[degree_idx, lambda_idx], scores1[degree_idx, lambda_idx]= cross_validation(labels1, estimated_data1, k_idx1, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score1:', scores1[degree_idx, lambda_idx])\n",
    "        print('Loss1:', loss_te1[degree_idx, lambda_idx])\n",
    "        \n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te2[degree_idx, lambda_idx], scores2[degree_idx, lambda_idx]= cross_validation(labels2, estimated_data2, k_idx2, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score2:', scores2[degree_idx, lambda_idx])\n",
    "        print('Loss2:', loss_te2[degree_idx, lambda_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree: 9 Best lambda: 0.001 Best score: 0.8404564107696927 Best loss: 0.07914623160844761\n",
      "Best degree: 9 Best lambda: 0.001 Best score: 0.8035852463244777 Best loss: 0.099316481815837\n",
      "Best degree: 10 Best lambda: 1e-07 Best score: 0.8276812792941826 Best loss: 0.08741384063964708\n"
     ]
    }
   ],
   "source": [
    "ratio0 = scores0/loss_te0\n",
    "ratio1 = scores1/loss_te1\n",
    "ratio2 = scores2/loss_te2\n",
    "\n",
    "best_HP_idx0 = np.unravel_index(np.argmax(ratio0), np.shape(ratio0))\n",
    "best_degree0 = degrees[best_HP_idx0[0]]\n",
    "best_lambda0 = lambdas[best_HP_idx0[1]]\n",
    "best_score0 = scores0[best_HP_idx0[0], best_HP_idx0[1]]\n",
    "best_loss0 = loss_te0[best_HP_idx0[0], best_HP_idx0[1]]\n",
    "print('Best degree:', best_degree0, 'Best lambda:', best_lambda0, 'Best score:', best_score0, 'Best loss:', best_loss0)\n",
    "\n",
    "best_HP_idx1 = np.unravel_index(np.argmax(ratio1), np.shape(ratio1))\n",
    "best_degree1 = degrees[best_HP_idx1[0]]\n",
    "best_lambda1 = lambdas[best_HP_idx1[1]]\n",
    "best_score1 = scores1[best_HP_idx1[0], best_HP_idx1[1]]\n",
    "best_loss1 = loss_te1[best_HP_idx1[0], best_HP_idx1[1]]\n",
    "print('Best degree:', best_degree1, 'Best lambda:', best_lambda1, 'Best score:', best_score1, 'Best loss:', best_loss1)\n",
    "\n",
    "best_HP_idx2 = np.unravel_index(np.argmax(ratio2), np.shape(ratio2))\n",
    "best_degree2 = degrees[best_HP_idx2[0]]\n",
    "best_lambda2 = lambdas[best_HP_idx2[1]]\n",
    "best_score2 = scores2[best_HP_idx2[0], best_HP_idx2[1]]\n",
    "best_loss2 = loss_te2[best_HP_idx2[0], best_HP_idx2[1]]\n",
    "print('Best degree:', best_degree2, 'Best lambda:', best_lambda2, 'Best score:', best_score2, 'Best loss:', best_loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to get weights and predict on test\n",
    "poly_data0 = build_poly(estimated_data0, best_degree0)\n",
    "weights0, loss0 = ridge_regression(labels0, poly_data0, best_lambda0)\n",
    "poly_data0_te = build_poly(estimated_data0_te, best_degree0)\n",
    "y_pred0 = predict_labels(weights0, poly_data0_te)\n",
    "\n",
    "poly_data1 = build_poly(estimated_data1, best_degree1)\n",
    "weights1, loss1 = ridge_regression(labels1, poly_data1, best_lambda1)\n",
    "poly_data1_te = build_poly(estimated_data1_te, best_degree1)\n",
    "y_pred1 = predict_labels(weights1, poly_data1_te)\n",
    "\n",
    "poly_data2 = build_poly(estimated_data2, best_degree2)\n",
    "weights2, loss2 = ridge_regression(labels2, poly_data2, best_lambda2)\n",
    "poly_data2_te = build_poly(estimated_data2_te, best_degree2)\n",
    "y_pred2 = predict_labels(weights2, poly_data2_te)\n",
    "\n",
    "#create_csv_submission(indices_te, y_pred_te, 'ridge_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_te[np.where(raw_data_te[:,22] == 0)[0]] = y_pred0\n",
    "labels_te[np.where(raw_data_te[:,22] == 1)[0]] = y_pred1\n",
    "labels_te[np.where(raw_data_te[:,22] > 1)[0]] = y_pred2\n",
    "\n",
    "create_csv_submission(indices_te, labels_te, 'test_jet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from proj1_helpers import *\n",
    "from Hugues import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "labels, raw_data, indices = load_csv_data('train.csv', sub_sample=False)\n",
    "nan_data = meaningless_to_nan(raw_data)\n",
    "\n",
    "labels_te, raw_data_te, indices_te = load_csv_data('test.csv', sub_sample=False)\n",
    "nan_data_te = meaningless_to_nan(raw_data_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate whole features\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing NaN [ 0  4  5  6 12 23 24 25 26 27 28]\n",
      "Columns containing NaN [ 0  4  5  6 12 23 24 25 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "# Estimation of columns for train set\n",
    "lambdas = np.logspace(-4,0,5)\n",
    "k_fold = 4\n",
    "\n",
    "estimated_data = column_estimation(nan_data, lambdas, k_fold)\n",
    "estimated_data_te = column_estimation(nan_data_te, lambdas, k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 0 Lambda: 0.1\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 0.01\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 0.001\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 0.0001\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-05\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-06\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-07\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-08\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-09\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 0 Lambda: 1e-10\n",
      "Score: 0.65752\n",
      "Loss: 0.5\n",
      "Degree: 1 Lambda: 0.1\n",
      "Score: 0.7318\n",
      "Loss: 0.5547739999999999\n",
      "Degree: 1 Lambda: 0.01\n",
      "Score: 0.74076\n",
      "Loss: 0.5447379999999999\n",
      "Degree: 1 Lambda: 0.001\n",
      "Score: 0.74668\n",
      "Loss: 0.538098\n",
      "Degree: 1 Lambda: 0.0001\n",
      "Score: 0.74728\n",
      "Loss: 0.53725\n",
      "Degree: 1 Lambda: 1e-05\n",
      "Score: 0.74756\n",
      "Loss: 0.537084\n",
      "Degree: 1 Lambda: 1e-06\n",
      "Score: 0.74764\n",
      "Loss: 0.537036\n",
      "Degree: 1 Lambda: 1e-07\n",
      "Score: 0.74764\n",
      "Loss: 0.5370820000000001\n",
      "Degree: 1 Lambda: 1e-08\n",
      "Score: 0.7476\n",
      "Loss: 0.53712\n",
      "Degree: 1 Lambda: 1e-09\n",
      "Score: 0.74756\n",
      "Loss: 0.5371060000000001\n",
      "Degree: 1 Lambda: 1e-10\n",
      "Score: 0.7476\n",
      "Loss: 0.5371180000000001\n",
      "Degree: 2 Lambda: 0.1\n",
      "Score: 0.75272\n",
      "Loss: 0.540116\n",
      "Degree: 2 Lambda: 0.01\n",
      "Score: 0.76232\n",
      "Loss: 0.53141\n",
      "Degree: 2 Lambda: 0.001\n",
      "Score: 0.76912\n",
      "Loss: 0.5241680000000001\n",
      "Degree: 2 Lambda: 0.0001\n",
      "Score: 0.77084\n",
      "Loss: 0.521646\n",
      "Degree: 2 Lambda: 1e-05\n",
      "Score: 0.7708\n",
      "Loss: 0.5212019999999999\n",
      "Degree: 2 Lambda: 1e-06\n",
      "Score: 0.77084\n",
      "Loss: 0.521166\n",
      "Degree: 2 Lambda: 1e-07\n",
      "Score: 0.7708\n",
      "Loss: 0.52118\n",
      "Degree: 2 Lambda: 1e-08\n",
      "Score: 0.77072\n",
      "Loss: 0.521162\n",
      "Degree: 2 Lambda: 1e-09\n",
      "Score: 0.77068\n",
      "Loss: 0.521228\n",
      "Degree: 2 Lambda: 1e-10\n",
      "Score: 0.77068\n",
      "Loss: 0.521242\n",
      "Degree: 3 Lambda: 0.1\n",
      "Score: 0.76052\n",
      "Loss: 0.536284\n",
      "Degree: 3 Lambda: 0.01\n",
      "Score: 0.76816\n",
      "Loss: 0.527486\n",
      "Degree: 3 Lambda: 0.001\n",
      "Score: 0.77296\n",
      "Loss: 0.52115\n",
      "Degree: 3 Lambda: 0.0001\n",
      "Score: 0.7764\n",
      "Loss: 0.5184179999999999\n",
      "Degree: 3 Lambda: 1e-05\n",
      "Score: 0.7776\n",
      "Loss: 0.517434\n",
      "Degree: 3 Lambda: 1e-06\n",
      "Score: 0.77768\n",
      "Loss: 0.5175240000000001\n",
      "Degree: 3 Lambda: 1e-07\n",
      "Score: 0.77756\n",
      "Loss: 0.5175\n",
      "Degree: 3 Lambda: 1e-08\n",
      "Score: 0.77752\n",
      "Loss: 0.517518\n",
      "Degree: 3 Lambda: 1e-09\n",
      "Score: 0.77764\n",
      "Loss: 0.51751\n",
      "Degree: 3 Lambda: 1e-10\n",
      "Score: 0.77764\n",
      "Loss: 0.5175080000000001\n",
      "Degree: 4 Lambda: 0.1\n",
      "Score: 0.7696\n",
      "Loss: 0.526922\n",
      "Degree: 4 Lambda: 0.01\n",
      "Score: 0.77628\n",
      "Loss: 0.52016\n",
      "Degree: 4 Lambda: 0.001\n",
      "Score: 0.78156\n",
      "Loss: 0.5140960000000001\n",
      "Degree: 4 Lambda: 0.0001\n",
      "Score: 0.78388\n",
      "Loss: 0.512242\n",
      "Degree: 4 Lambda: 1e-05\n",
      "Score: 0.78484\n",
      "Loss: 0.512226\n",
      "Degree: 4 Lambda: 1e-06\n",
      "Score: 0.785\n",
      "Loss: 0.512284\n",
      "Degree: 4 Lambda: 1e-07\n",
      "Score: 0.78508\n",
      "Loss: 0.5122639999999999\n",
      "Degree: 4 Lambda: 1e-08\n",
      "Score: 0.78528\n",
      "Loss: 0.512206\n",
      "Degree: 4 Lambda: 1e-09\n",
      "Score: 0.7852\n",
      "Loss: 0.5121960000000001\n",
      "Degree: 4 Lambda: 1e-10\n",
      "Score: 0.7852\n",
      "Loss: 0.5121960000000001\n",
      "Degree: 5 Lambda: 0.1\n",
      "Score: 0.77812\n",
      "Loss: 0.519914\n",
      "Degree: 5 Lambda: 0.01\n",
      "Score: 0.78296\n",
      "Loss: 0.516074\n",
      "Degree: 5 Lambda: 0.001\n",
      "Score: 0.78788\n",
      "Loss: 0.5112680000000001\n",
      "Degree: 5 Lambda: 0.0001\n",
      "Score: 0.79008\n",
      "Loss: 0.510038\n",
      "Degree: 5 Lambda: 1e-05\n",
      "Score: 0.791\n",
      "Loss: 0.509636\n",
      "Degree: 5 Lambda: 1e-06\n",
      "Score: 0.7916\n",
      "Loss: 0.5095679999999999\n",
      "Degree: 5 Lambda: 1e-07\n",
      "Score: 0.79172\n",
      "Loss: 0.509468\n",
      "Degree: 5 Lambda: 1e-08\n",
      "Score: 0.79188\n",
      "Loss: 0.509342\n",
      "Degree: 5 Lambda: 1e-09\n",
      "Score: 0.79204\n",
      "Loss: 0.5093\n",
      "Degree: 5 Lambda: 1e-10\n",
      "Score: 0.79204\n",
      "Loss: 0.5092899999999999\n",
      "Degree: 6 Lambda: 0.1\n",
      "Score: 0.78492\n",
      "Loss: 0.5149279999999998\n",
      "Degree: 6 Lambda: 0.01\n",
      "Score: 0.7876\n",
      "Loss: 0.5136679999999999\n",
      "Degree: 6 Lambda: 0.001\n",
      "Score: 0.79116\n",
      "Loss: 0.511294\n",
      "Degree: 6 Lambda: 0.0001\n",
      "Score: 0.79292\n",
      "Loss: 0.5102040000000001\n",
      "Degree: 6 Lambda: 1e-05\n",
      "Score: 0.794\n",
      "Loss: 0.510108\n",
      "Degree: 6 Lambda: 1e-06\n",
      "Score: 0.79392\n",
      "Loss: 0.5100140000000001\n",
      "Degree: 6 Lambda: 1e-07\n",
      "Score: 0.79396\n",
      "Loss: 0.5099020000000001\n",
      "Degree: 6 Lambda: 1e-08\n",
      "Score: 0.79388\n",
      "Loss: 0.509892\n",
      "Degree: 6 Lambda: 1e-09\n",
      "Score: 0.79424\n",
      "Loss: 0.509962\n",
      "Degree: 6 Lambda: 1e-10\n",
      "Score: 0.79416\n",
      "Loss: 0.510006\n",
      "Degree: 7 Lambda: 0.1\n",
      "Score: 0.78852\n",
      "Loss: 0.511768\n",
      "Degree: 7 Lambda: 0.01\n",
      "Score: 0.78996\n",
      "Loss: 0.510952\n",
      "Degree: 7 Lambda: 0.001\n",
      "Score: 0.79188\n",
      "Loss: 0.5105839999999999\n",
      "Degree: 7 Lambda: 0.0001\n",
      "Score: 0.79356\n",
      "Loss: 0.5103139999999999\n",
      "Degree: 7 Lambda: 1e-05\n",
      "Score: 0.79316\n",
      "Loss: 0.510286\n",
      "Degree: 7 Lambda: 1e-06\n",
      "Score: 0.7938\n",
      "Loss: 0.5099899999999999\n",
      "Degree: 7 Lambda: 1e-07\n",
      "Score: 0.79484\n",
      "Loss: 0.50977\n",
      "Degree: 7 Lambda: 1e-08\n",
      "Score: 0.79564\n",
      "Loss: 0.50957\n",
      "Degree: 7 Lambda: 1e-09\n",
      "Score: 0.79576\n",
      "Loss: 0.509426\n",
      "Degree: 7 Lambda: 1e-10\n",
      "Score: 0.79604\n",
      "Loss: 0.5093799999999999\n",
      "Degree: 8 Lambda: 0.1\n",
      "Score: 0.7936\n",
      "Loss: 0.50783\n",
      "Degree: 8 Lambda: 0.01\n",
      "Score: 0.79492\n",
      "Loss: 0.508028\n",
      "Degree: 8 Lambda: 0.001\n",
      "Score: 0.79576\n",
      "Loss: 0.5084980000000001\n",
      "Degree: 8 Lambda: 0.0001\n",
      "Score: 0.79648\n",
      "Loss: 0.508732\n",
      "Degree: 8 Lambda: 1e-05\n",
      "Score: 0.79744\n",
      "Loss: 0.509176\n",
      "Degree: 8 Lambda: 1e-06\n",
      "Score: 0.79736\n",
      "Loss: 0.509646\n",
      "Degree: 8 Lambda: 1e-07\n",
      "Score: 0.79856\n",
      "Loss: 0.510156\n",
      "Degree: 8 Lambda: 1e-08\n",
      "Score: 0.79836\n",
      "Loss: 0.511312\n",
      "Degree: 8 Lambda: 1e-09\n",
      "Score: 0.7984\n",
      "Loss: 0.511398\n",
      "Degree: 8 Lambda: 1e-10\n",
      "Score: 0.79868\n",
      "Loss: 0.5110659999999999\n",
      "Degree: 9 Lambda: 0.1\n",
      "Score: 0.79964\n",
      "Loss: 0.502612\n",
      "Degree: 9 Lambda: 0.01\n",
      "Score: 0.80156\n",
      "Loss: 0.500714\n",
      "Degree: 9 Lambda: 0.001\n",
      "Score: 0.80348\n",
      "Loss: 0.49991199999999997\n",
      "Degree: 9 Lambda: 0.0001\n",
      "Score: 0.80392\n",
      "Loss: 0.499822\n",
      "Degree: 9 Lambda: 1e-05\n",
      "Score: 0.80344\n",
      "Loss: 0.49962399999999996\n",
      "Degree: 9 Lambda: 1e-06\n",
      "Score: 0.80376\n",
      "Loss: 0.49991199999999997\n",
      "Degree: 9 Lambda: 1e-07\n",
      "Score: 0.8038\n",
      "Loss: 0.49993799999999994\n",
      "Degree: 9 Lambda: 1e-08\n",
      "Score: 0.804\n",
      "Loss: 0.500076\n",
      "Degree: 9 Lambda: 1e-09\n",
      "Score: 0.80412\n",
      "Loss: 0.500202\n",
      "Degree: 9 Lambda: 1e-10\n",
      "Score: 0.80484\n",
      "Loss: 0.5001819999999999\n",
      "Degree: 10 Lambda: 0.1\n",
      "Score: 0.80444\n",
      "Loss: 0.5005\n",
      "Degree: 10 Lambda: 0.01\n",
      "Score: 0.80648\n",
      "Loss: 0.49835599999999997\n",
      "Degree: 10 Lambda: 0.001\n",
      "Score: 0.80764\n",
      "Loss: 0.49720799999999993\n",
      "Degree: 10 Lambda: 0.0001\n",
      "Score: 0.80928\n",
      "Loss: 0.496286\n",
      "Degree: 10 Lambda: 1e-05\n",
      "Score: 0.80968\n",
      "Loss: 0.49618400000000007\n",
      "Degree: 10 Lambda: 1e-06\n",
      "Score: 0.81028\n",
      "Loss: 0.49985999999999997\n",
      "Degree: 10 Lambda: 1e-07\n",
      "Score: 0.81092\n",
      "Loss: 0.497742\n",
      "Degree: 10 Lambda: 1e-08\n",
      "Score: 0.81088\n",
      "Loss: 0.49821600000000005\n",
      "Degree: 10 Lambda: 1e-09\n",
      "Score: 0.81124\n",
      "Loss: 0.497316\n",
      "Degree: 10 Lambda: 1e-10\n",
      "Score: 0.811\n",
      "Loss: 0.49876200000000004\n",
      "Degree: 11 Lambda: 0.1\n",
      "Score: 0.80512\n",
      "Loss: 0.5072319999999999\n",
      "Degree: 11 Lambda: 0.01\n",
      "Score: 0.80972\n",
      "Loss: 0.505024\n",
      "Degree: 11 Lambda: 0.001\n",
      "Score: 0.81152\n",
      "Loss: 0.5309360000000001\n",
      "Degree: 11 Lambda: 0.0001\n",
      "Score: 0.81312\n",
      "Loss: 0.51032\n",
      "Degree: 11 Lambda: 1e-05\n",
      "Score: 0.81368\n",
      "Loss: 0.5195000000000001\n",
      "Degree: 11 Lambda: 1e-06\n",
      "Score: 0.81384\n",
      "Loss: 0.554682\n",
      "Degree: 11 Lambda: 1e-07\n",
      "Score: 0.8138\n",
      "Loss: 0.54254\n",
      "Degree: 11 Lambda: 1e-08\n",
      "Score: 0.8098\n",
      "Loss: 0.559748\n",
      "Degree: 11 Lambda: 1e-09\n",
      "Score: 0.75004\n",
      "Loss: 0.5280759999999999\n",
      "Degree: 11 Lambda: 1e-10\n",
      "Score: 0.77484\n",
      "Loss: 0.554864\n",
      "Degree: 12 Lambda: 0.1\n",
      "Score: 0.80428\n",
      "Loss: 0.50534\n",
      "Degree: 12 Lambda: 0.01\n",
      "Score: 0.80848\n",
      "Loss: 0.5029980000000001\n",
      "Degree: 12 Lambda: 0.001\n",
      "Score: 0.81004\n",
      "Loss: 0.523346\n",
      "Degree: 12 Lambda: 0.0001\n",
      "Score: 0.81128\n",
      "Loss: 0.502284\n",
      "Degree: 12 Lambda: 1e-05\n",
      "Score: 0.81256\n",
      "Loss: 0.537768\n",
      "Degree: 12 Lambda: 1e-06\n",
      "Score: 0.81108\n",
      "Loss: 0.564566\n",
      "Degree: 12 Lambda: 1e-07\n",
      "Score: 0.73584\n",
      "Loss: 0.5631920000000001\n",
      "Degree: 12 Lambda: 1e-08\n",
      "Score: 0.64608\n",
      "Loss: 0.635646\n",
      "Degree: 12 Lambda: 1e-09\n",
      "Score: 0.63444\n",
      "Loss: 0.65241\n",
      "Degree: 12 Lambda: 1e-10\n",
      "Score: 0.73028\n",
      "Loss: 0.6410180000000001\n",
      "Degree: 13 Lambda: 0.1\n",
      "Score: 0.80428\n",
      "Loss: 0.5221640000000001\n",
      "Degree: 13 Lambda: 0.01\n",
      "Score: 0.79632\n",
      "Loss: 0.511798\n",
      "Degree: 13 Lambda: 0.001\n",
      "Score: 0.61096\n",
      "Loss: 0.5772600000000001\n",
      "Degree: 13 Lambda: 0.0001\n",
      "Score: 0.60276\n",
      "Loss: 0.59273\n",
      "Degree: 13 Lambda: 1e-05\n",
      "Score: 0.549\n",
      "Loss: 0.5884079999999999\n",
      "Degree: 13 Lambda: 1e-06\n",
      "Score: 0.49736\n",
      "Loss: 0.664796\n",
      "Degree: 13 Lambda: 1e-07\n",
      "Score: 0.63108\n",
      "Loss: 0.605844\n",
      "Degree: 13 Lambda: 1e-08\n",
      "Score: 0.52304\n",
      "Loss: 0.6618700000000001\n",
      "Degree: 13 Lambda: 1e-09\n",
      "Score: 0.59268\n",
      "Loss: 0.690704\n",
      "Degree: 13 Lambda: 1e-10\n",
      "Score: 0.71224\n",
      "Loss: 0.682886\n",
      "Degree: 14 Lambda: 0.1\n",
      "Score: 0.80336\n",
      "Loss: 0.5530799999999999\n",
      "Degree: 14 Lambda: 0.01\n",
      "Score: 0.80764\n",
      "Loss: 0.592198\n",
      "Degree: 14 Lambda: 0.001\n",
      "Score: 0.80332\n",
      "Loss: 0.55629\n",
      "Degree: 14 Lambda: 0.0001\n",
      "Score: 0.80232\n",
      "Loss: 0.6513720000000001\n",
      "Degree: 14 Lambda: 1e-05\n",
      "Score: 0.752\n",
      "Loss: 0.5722599999999999\n",
      "Degree: 14 Lambda: 1e-06\n",
      "Score: 0.54044\n",
      "Loss: 0.5864360000000002\n",
      "Degree: 14 Lambda: 1e-07\n",
      "Score: 0.75864\n",
      "Loss: 0.6977059999999999\n",
      "Degree: 14 Lambda: 1e-08\n",
      "Score: 0.79968\n",
      "Loss: 0.7045\n",
      "Degree: 14 Lambda: 1e-09\n",
      "Score: 0.79324\n",
      "Loss: 0.6585139999999999\n",
      "Degree: 14 Lambda: 1e-10\n",
      "Score: 0.71904\n",
      "Loss: 0.6803600000000001\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "degrees = range(15)\n",
    "lambdas = np.logspace(-1, -10, 10)\n",
    "k_fold = 10\n",
    "seed = 42\n",
    "\n",
    "k_idx = build_k_indices(labels, k_fold, seed)\n",
    "loss_te = np.ones((len(degrees),len(lambdas)))\n",
    "scores = np.ones((len(degrees),len(lambdas)))\n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te[degree_idx, lambda_idx], scores[degree_idx, lambda_idx]= cross_validation_(labels, estimated_data, k_idx, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score:', scores[degree_idx, lambda_idx])\n",
    "        print('Loss:', loss_te[degree_idx, lambda_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n",
      "Best degree: 10 Best lambda: 1e-05 Best score: 0.80968 Best loss 0.49618400000000007\n"
     ]
    }
   ],
   "source": [
    "ratio = scores/loss_te\n",
    "print(ratio.shape)\n",
    "best_HP_idx = np.unravel_index(np.argmax(ratio), np.shape(ratio))\n",
    "best_degree = degrees[best_HP_idx[0]]\n",
    "best_lambda = lambdas[best_HP_idx[1]]\n",
    "best_score = scores[best_HP_idx[0], best_HP_idx[1]]\n",
    "best_loss = loss_te[best_HP_idx[0], best_HP_idx[1]]\n",
    "\n",
    "print('Best degree:', best_degree, 'Best lambda:', best_lambda, 'Best score:', best_score, 'Best loss', best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to get weights\n",
    "poly_estimated_data = build_poly(estimated_data, best_degree)\n",
    "weights, loss = ridge_regression(labels, poly_estimated_data, best_lambda)\n",
    "\n",
    "#Predict on test\n",
    "poly_estimated_data_te = build_poly(estimated_data_te, best_degree)\n",
    "y_pred_te = predict_labels(weights, poly_estimated_data_te)\n",
    "\n",
    "create_csv_submission(indices_te, y_pred_te, 'csv_de_l_angoisse.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

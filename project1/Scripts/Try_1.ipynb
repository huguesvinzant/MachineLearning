{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from used_functions import *\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "labels, raw_data, indices = load_csv_data('train.csv', sub_sample=False)\n",
    "nan_data = meaningless_to_nan(raw_data)\n",
    "\n",
    "labels_te, raw_data_te, indices_te = load_csv_data('test.csv', sub_sample=False)\n",
    "nan_data_te = meaningless_to_nan(raw_data_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  4  5  6 12 23 24 25 26 27 28]\n",
      "[ 0  4  5  6 12 23 24 25 26 27 28]\n",
      "[ 0  4  5  6 12 23 24 25 26 27 28]\n",
      "[ 0  4  5  6 12 23 24 25 26 27 28]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "n_samples, n_features = np.shape(nan_data)\n",
    "nan_columns = nan_find_columns(n_features, n_samples, nan_data)\n",
    "\n",
    "#Test\n",
    "n_samples_te, n_features_te = np.shape(raw_data_te)\n",
    "nan_columns_te = nan_find_columns(n_features_te, n_samples_te, nan_data_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate whole features\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "1e-05\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "1e-05\n",
      "1e-05\n",
      "0.001\n",
      "0.001\n",
      "0.01\n",
      "0.01\n",
      "0.001\n",
      "0.001\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "# Estimation of columns for train set\n",
    "new_submatrix = np.delete(nan_data, nan_columns, axis = 1)\n",
    "k_fold = 4\n",
    "seed = 2\n",
    "lambdas = np.logspace(-5,-1,5)\n",
    "loss_te = np.ones(len(lambdas))\n",
    "loss_tr = np.ones(len(lambdas))\n",
    "\n",
    "for chosen_feature in nan_columns:\n",
    "    samples = []\n",
    "    for sample in range(n_samples):\n",
    "        if np.isnan(nan_data[sample,chosen_feature]):\n",
    "            samples.append(sample)\n",
    "    nan_lines = samples\n",
    "    new_submatrix0 = np.delete(new_submatrix,nan_lines, axis = 0)\n",
    "    labels_0 = np.delete(nan_data[:,chosen_feature],nan_lines, axis = 0)\n",
    "    k_indices = build_k_indices(labels_0, k_fold, seed)\n",
    "    for idx, lambda_ in enumerate(lambdas):\n",
    "        loss_tr[idx],loss_te[idx] = cross_validation(labels_0, new_submatrix0, k_indices, k_fold, lambda_, degree=1)\n",
    "    best_lambda = lambdas[np.argmin(loss_te)]\n",
    "    print((best_lambda))\n",
    "    weights, loss = ridge_regression(labels_0, new_submatrix0, best_lambda)\n",
    "    x_pred = np.dot(new_submatrix[nan_lines,:], weights)\n",
    "    nan_data[nan_lines,chosen_feature] = x_pred\n",
    "    #print(x_pred.shape)\n",
    "    \n",
    "estimated_data = nan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.01\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "# Estimation of columns for test set\n",
    "new_submatrix_te = np.delete(nan_data_te,nan_columns, axis = 1)\n",
    "loss_te_ = np.ones(len(lambdas))\n",
    "loss_tr_ = np.ones(len(lambdas))\n",
    "for chosen_feature in nan_columns:\n",
    "    samples_te = []\n",
    "    for sample in range(n_samples_te):\n",
    "        if np.isnan(nan_data_te[sample,chosen_feature]):\n",
    "            samples_te.append(sample)\n",
    "    nan_lines_te = samples_te\n",
    "    new_submatrix0_te = np.delete(new_submatrix_te,nan_lines_te, axis = 0)\n",
    "    labels_0_te = np.delete(nan_data_te[:,chosen_feature],nan_lines_te, axis = 0)\n",
    "    k_indices_te = build_k_indices(labels_0_te, k_fold, seed)\n",
    "    for idx, lambda_ in enumerate(lambdas):\n",
    "        loss_tr_[idx],loss_te_[idx] = cross_validation(labels_0_te, new_submatrix0_te, k_indices_te, k_fold, lambda_, degree=1)\n",
    "    best_lambda_te = lambdas[np.argmin(loss_te)]\n",
    "    print((best_lambda_te))\n",
    "    weights_te, loss_te = ridge_regression(labels_0, new_submatrix0, best_lambda)\n",
    "    x_pred_te = np.dot(new_submatrix_te[nan_lines_te,:], weights_te)\n",
    "    nan_data_te[nan_lines_te,chosen_feature] = x_pred_te\n",
    "    #print(x_pred.shape)\n",
    "estimated_data_te = nan_data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best parameters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 6 Lambda: 1e-09\n",
      "Score: 0.79768\n",
      "Loss: 69123072.61699656\n",
      "Class score: 0.7992911968859009\n",
      "Degree: 6 Lambda: 1e-09\n",
      "Score: 0.79768\n",
      "Loss: 69123072.61699656\n",
      "Class score: 0.7992911968859009\n",
      "Degree: 6 Lambda: 2.7825594022071257e-09\n",
      "Score: 0.797648\n",
      "Loss: 68877792.67222956\n",
      "Class score: 0.7992598641478896\n",
      "Degree: 6 Lambda: 2.7825594022071257e-09\n",
      "Score: 0.797648\n",
      "Loss: 68877792.67222956\n",
      "Class score: 0.7992598641478896\n",
      "Degree: 6 Lambda: 7.742636826811278e-09\n",
      "Score: 0.79752\n",
      "Loss: 68418629.44779992\n",
      "Class score: 0.7991330156980533\n",
      "Degree: 6 Lambda: 7.742636826811278e-09\n",
      "Score: 0.79752\n",
      "Loss: 68418629.44779992\n",
      "Class score: 0.7991330156980533\n",
      "Degree: 6 Lambda: 2.1544346900318822e-08\n",
      "Score: 0.797568\n",
      "Loss: 67837722.83571076\n",
      "Class score: 0.7991810264702643\n",
      "Degree: 6 Lambda: 2.1544346900318822e-08\n",
      "Score: 0.797568\n",
      "Loss: 67837722.83571076\n",
      "Class score: 0.7991810264702643\n",
      "Degree: 6 Lambda: 5.99484250318941e-08\n",
      "Score: 0.797584\n",
      "Loss: 67378777.05858064\n",
      "Class score: 0.7991971986718669\n",
      "Degree: 6 Lambda: 5.99484250318941e-08\n",
      "Score: 0.797584\n",
      "Loss: 67378777.05858064\n",
      "Class score: 0.7991971986718669\n",
      "Degree: 6 Lambda: 1.668100537200059e-07\n",
      "Score: 0.797648\n",
      "Loss: 67126911.48281549\n",
      "Class score: 0.7992634049760686\n",
      "Degree: 6 Lambda: 1.668100537200059e-07\n",
      "Score: 0.797648\n",
      "Loss: 67126911.48281549\n",
      "Class score: 0.7992634049760686\n",
      "Degree: 6 Lambda: 4.6415888336127725e-07\n",
      "Score: 0.797712\n",
      "Loss: 67001950.37780339\n",
      "Class score: 0.7993296112802704\n",
      "Degree: 6 Lambda: 4.6415888336127725e-07\n",
      "Score: 0.797712\n",
      "Loss: 67001950.37780339\n",
      "Class score: 0.7993296112802704\n",
      "Degree: 6 Lambda: 1.2915496650148827e-06\n",
      "Score: 0.79792\n",
      "Loss: 66916229.82391098\n",
      "Class score: 0.7995352974077319\n",
      "Degree: 6 Lambda: 1.2915496650148827e-06\n",
      "Score: 0.79792\n",
      "Loss: 66916229.82391098\n",
      "Class score: 0.7995352974077319\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "degrees = [6,7,8]\n",
    "lambdas = np.logspace(-9, -5, 10)\n",
    "k_fold = 4\n",
    "seed = 2\n",
    "\n",
    "k_idx = build_k_indices(labels, k_fold, seed)\n",
    "#loss_tr = np.ones((len(degrees),len(lambdas)))\n",
    "loss_te = np.ones((len(degrees),len(lambdas)))\n",
    "scores = np.ones((len(degrees),len(lambdas)))\n",
    "class_scores = np.ones((len(degrees),len(lambdas)))\n",
    "#labels = np.expand_dims(labels, axis=1) \n",
    "\n",
    "for degree_idx, degree in enumerate(degrees):\n",
    "    for lambda_idx, lambda_ in enumerate(lambdas):\n",
    "        _ ,loss_te[degree_idx, lambda_idx], scores[degree_idx, lambda_idx], class_scores[degree_idx, lambda_idx] = cross_validation_(labels, estimated_data, k_idx, k_fold, lambda_, degree)\n",
    "        print('Degree:', degrees[degree_idx], 'Lambda:', lambdas[lambda_idx])\n",
    "        print('Score:', scores[degree_idx, lambda_idx])\n",
    "        print('Loss:', loss_te[degree_idx, lambda_idx])\n",
    "        print('Class score:', class_scores[degree_idx, lambda_idx])\n",
    "        \n",
    "#Error        \n",
    "best_HP_idx = np.unravel_index(np.argmax(scores), np.shape(scores))\n",
    "best_degree = degrees[best_HP_idx[0]]\n",
    "best_lambda = lambdas[best_HP_idx[1]]\n",
    "best_score = scores[best_HP_idx[0], best_HP_idx[1]]\n",
    "#Class error\n",
    "best_HP_idx_class = np.unravel_index(np.argmax(class_scores), np.shape(class_scores))\n",
    "best_degree_class = degrees[best_HP_idx_class[0]]\n",
    "best_lambda_class = lambdas[best_HP_idx_class[1]]\n",
    "best_class_score = class_scores[best_HP_idx_class[0], best_HP_idx_class[1]]\n",
    "\n",
    "\n",
    "print('Best degree:', best_degree, 'Best lambda:', best_lambda, 'Best score:', best_score, 'Best class score:', best_class_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to get weights\n",
    "poly_estimated_data = build_poly(estimated_data, best_degree_class )\n",
    "weights, loss = ridge_regression(labels, poly_estimated_data, best_lambda_class)\n",
    "\n",
    "#Predict on test\n",
    "poly_estimated_data_te = build_poly(estimated_data_te, best_degree_class)\n",
    "y_pred_te = predict_labels(weights, poly_estimated_data_te)\n",
    "\n",
    "accuracy_ =accuracy (y_pred_te,labels_te)\n",
    "print(accuracy_)\n",
    "#print(np.shape(y_pred_te), np.shape(indices_te))\n",
    "#create_csv_submission(indices_te, y_pred_te, 'submission_features_augmentation_ridgereg_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
